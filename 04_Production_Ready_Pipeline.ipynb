{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "9abed6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pytest in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (8.4.1)\n",
      "Requirement already satisfied: iniconfig>=1 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pytest) (24.2)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pytest) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pytest) (2.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1c02663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.2)\n",
      "Requirement already satisfied: pandas in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (1.7.1)\n",
      "Requirement already satisfied: openai in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (1.97.1)\n",
      "Requirement already satisfied: groq in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (0.30.0)\n",
      "Requirement already satisfied: seaborn in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: dotenv in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (0.9.9)\n",
      "Requirement already satisfied: matplotlib in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (3.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from openai->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from dotenv->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 8)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 8)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 8)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: reportlab in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (4.4.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from reportlab) (11.3.0)\n",
      "Requirement already satisfied: charset-normalizer in /Users/kavishkathilakarathna/miniconda3/lib/python3.13/site-packages (from reportlab) (3.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "# Install reportlab in your environment\n",
    "!pip install reportlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fd6dca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytest\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "2c26df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae65931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b227fb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "      <th>Churn_Prob</th>\n",
       "      <th>Retention_Segment</th>\n",
       "      <th>Retention_Priority</th>\n",
       "      <th>Risk_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353677</td>\n",
       "      <td>Medium-Risk (Engage)</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "      <td>0.248558</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415959</td>\n",
       "      <td>Medium-Risk (Engage)</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.159968</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "      <td>0.550283</td>\n",
       "      <td>Medium-Risk (Engage)</td>\n",
       "      <td>High</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110945</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.509897</td>\n",
       "      <td>Medium-Risk (Engage)</td>\n",
       "      <td>High</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177270</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198290</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376237</td>\n",
       "      <td>Medium-Risk (Engage)</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.02</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.03299</td>\n",
       "      <td>0.03323</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "      <td>0</td>\n",
       "      <td>0.543467</td>\n",
       "      <td>Medium-Risk (Engage)</td>\n",
       "      <td>High</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.78</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.09954</td>\n",
       "      <td>0.06606</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5609</td>\n",
       "      <td>0.3965</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084764</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.17</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.20650</td>\n",
       "      <td>0.11180</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.3639</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357929</td>\n",
       "      <td>Medium-Risk (Engage)</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.85</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.09938</td>\n",
       "      <td>0.05364</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.2322</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273321</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.73</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.21280</td>\n",
       "      <td>0.08025</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.6943</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134376</td>\n",
       "      <td>Low-Risk (Retain)</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         17.99         10.38          122.80     1001.0          0.11840   \n",
       "1         20.57         17.77          132.90     1326.0          0.08474   \n",
       "2         19.69         21.25          130.00     1203.0          0.10960   \n",
       "3         11.42         20.38           77.58      386.1          0.14250   \n",
       "4         20.29         14.34          135.10     1297.0          0.10030   \n",
       "5         12.45         15.70           82.57      477.1          0.12780   \n",
       "6         18.25         19.98          119.60     1040.0          0.09463   \n",
       "7         13.71         20.83           90.20      577.9          0.11890   \n",
       "8         13.00         21.82           87.50      519.8          0.12730   \n",
       "9         12.46         24.04           83.97      475.9          0.11860   \n",
       "10        16.02         23.24          102.70      797.8          0.08206   \n",
       "11        15.78         17.89          103.60      781.0          0.09710   \n",
       "12        19.17         24.80          132.40     1123.0          0.09740   \n",
       "13        15.85         23.95          103.70      782.7          0.08401   \n",
       "14        13.73         22.61           93.60      578.3          0.11310   \n",
       "\n",
       "    mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.27760         0.30010              0.14710         0.2419   \n",
       "1            0.07864         0.08690              0.07017         0.1812   \n",
       "2            0.15990         0.19740              0.12790         0.2069   \n",
       "3            0.28390         0.24140              0.10520         0.2597   \n",
       "4            0.13280         0.19800              0.10430         0.1809   \n",
       "5            0.17000         0.15780              0.08089         0.2087   \n",
       "6            0.10900         0.11270              0.07400         0.1794   \n",
       "7            0.16450         0.09366              0.05985         0.2196   \n",
       "8            0.19320         0.18590              0.09353         0.2350   \n",
       "9            0.23960         0.22730              0.08543         0.2030   \n",
       "10           0.06669         0.03299              0.03323         0.1528   \n",
       "11           0.12920         0.09954              0.06606         0.1842   \n",
       "12           0.24580         0.20650              0.11180         0.2397   \n",
       "13           0.10020         0.09938              0.05364         0.1847   \n",
       "14           0.22930         0.21280              0.08025         0.2069   \n",
       "\n",
       "    mean fractal dimension  ...  worst compactness  worst concavity  \\\n",
       "0                  0.07871  ...             0.6656           0.7119   \n",
       "1                  0.05667  ...             0.1866           0.2416   \n",
       "2                  0.05999  ...             0.4245           0.4504   \n",
       "3                  0.09744  ...             0.8663           0.6869   \n",
       "4                  0.05883  ...             0.2050           0.4000   \n",
       "5                  0.07613  ...             0.5249           0.5355   \n",
       "6                  0.05742  ...             0.2576           0.3784   \n",
       "7                  0.07451  ...             0.3682           0.2678   \n",
       "8                  0.07389  ...             0.5401           0.5390   \n",
       "9                  0.08243  ...             1.0580           1.1050   \n",
       "10                 0.05697  ...             0.1551           0.1459   \n",
       "11                 0.06082  ...             0.5609           0.3965   \n",
       "12                 0.07800  ...             0.3903           0.3639   \n",
       "13                 0.05338  ...             0.1924           0.2322   \n",
       "14                 0.07682  ...             0.7725           0.6943   \n",
       "\n",
       "    worst concave points  worst symmetry  worst fractal dimension  target  \\\n",
       "0                0.26540          0.4601                  0.11890       0   \n",
       "1                0.18600          0.2750                  0.08902       0   \n",
       "2                0.24300          0.3613                  0.08758       0   \n",
       "3                0.25750          0.6638                  0.17300       0   \n",
       "4                0.16250          0.2364                  0.07678       0   \n",
       "5                0.17410          0.3985                  0.12440       0   \n",
       "6                0.19320          0.3063                  0.08368       0   \n",
       "7                0.15560          0.3196                  0.11510       0   \n",
       "8                0.20600          0.4378                  0.10720       0   \n",
       "9                0.22100          0.4366                  0.20750       0   \n",
       "10               0.09975          0.2948                  0.08452       0   \n",
       "11               0.18100          0.3792                  0.10480       0   \n",
       "12               0.17670          0.3176                  0.10230       0   \n",
       "13               0.11190          0.2809                  0.06287       0   \n",
       "14               0.22080          0.3596                  0.14310       0   \n",
       "\n",
       "    Churn_Prob     Retention_Segment  Retention_Priority  Risk_Segment  \n",
       "0     0.353677  Medium-Risk (Engage)              Medium           Low  \n",
       "1     0.248558     Low-Risk (Retain)                 Low           Low  \n",
       "2     0.415959  Medium-Risk (Engage)              Medium        Medium  \n",
       "3     0.159968     Low-Risk (Retain)                 Low           Low  \n",
       "4     0.550283  Medium-Risk (Engage)                High        Medium  \n",
       "5     0.110945     Low-Risk (Retain)                 Low           Low  \n",
       "6     0.509897  Medium-Risk (Engage)                High        Medium  \n",
       "7     0.177270     Low-Risk (Retain)                 Low           Low  \n",
       "8     0.198290     Low-Risk (Retain)                 Low           Low  \n",
       "9     0.376237  Medium-Risk (Engage)              Medium           Low  \n",
       "10    0.543467  Medium-Risk (Engage)                High        Medium  \n",
       "11    0.084764     Low-Risk (Retain)                 Low           Low  \n",
       "12    0.357929  Medium-Risk (Engage)              Medium           Low  \n",
       "13    0.273321     Low-Risk (Retain)                 Low           Low  \n",
       "14    0.134376     Low-Risk (Retain)                 Low           Low  \n",
       "\n",
       "[15 rows x 35 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Processed/MiniProjectS_Model_Evaluation.csv')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2612d29",
   "metadata": {},
   "source": [
    "4.1.1 Data Processing Modules\n",
    "\n",
    "\n",
    "4.1.2 Model Development Modules\n",
    "\n",
    "\n",
    "4.1.3 Pipeline Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c7ba4c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading dataset...\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.85      1033\n",
      "           1       0.00      0.00      0.00       374\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.37      0.50      0.42      1407\n",
      "weighted avg       0.54      0.73      0.62      1407\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Model saved to output/artifacts/churn_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'Not Churn', 'probability': np.float64(0.427662040901877)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Directories\n",
    "Path(\"output/charts\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"output/artifacts\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"model_params\": {\"n_estimators\": 200, \"max_depth\": 8, \"random_state\": 42},\n",
    "    \"target\": \"Churn\"\n",
    "}\n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "        self.df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        logging.info(\"Loading dataset...\")\n",
    "        self.df = pd.read_csv(self.path)\n",
    "        return self.df\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        self.df = self.df.replace(\" \", np.nan).dropna()\n",
    "        # Chart: Missing values\n",
    "        missing_counts = self.df.isnull().sum()\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.barplot(x=missing_counts.index, y=missing_counts.values)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(\"Missing Values per Feature\")\n",
    "        plt.savefig(\"output/charts/missing_values.png\")\n",
    "        plt.close()\n",
    "        return self.df\n",
    "\n",
    "    def detect_outliers(self, col):\n",
    "        Q1, Q3 = self.df[col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        outliers = self.df[(self.df[col] < lower) | (self.df[col] > upper)]\n",
    "        # Chart: Boxplot\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.boxplot(x=self.df[col])\n",
    "        plt.title(f\"Outlier Detection: {col}\")\n",
    "        plt.savefig(f\"output/charts/outliers_{col}.png\")\n",
    "        plt.close()\n",
    "        return outliers\n",
    "\n",
    "    def add_tenure_bins(self):\n",
    "        bins = [0, 12, 24, 48, 72]\n",
    "        labels = [\"New\", \"Established\", \"Loyal\", \"Very Loyal\"]\n",
    "        self.df[\"TenureCategory\"] = pd.cut(self.df[\"tenure\"], bins=bins, labels=labels, right=False)\n",
    "        # Chart: Tenure distribution\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.countplot(x=\"TenureCategory\", data=self.df, palette=\"Set2\")\n",
    "        plt.title(\"Tenure Categories\")\n",
    "        plt.savefig(\"output/charts/tenure_bins.png\")\n",
    "        plt.close()\n",
    "        return self.df\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.df.drop(columns=[CONFIG[\"target\"], \"customerID\"])\n",
    "        y = self.df[CONFIG[\"target\"]].apply(lambda x: 1 if str(x).lower()==\"yes\" else 0)\n",
    "        # Chart: Target distribution\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.countplot(x=y, palette=\"pastel\")\n",
    "        plt.title(\"Churn Distribution\")\n",
    "        plt.savefig(\"output/charts/churn_distribution.png\")\n",
    "        plt.close()\n",
    "        return train_test_split(X, y, test_size=CONFIG[\"test_size\"], random_state=CONFIG[\"random_state\"], stratify=y)\n",
    "\n",
    "\n",
    "class ModelPipeline:\n",
    "    def __init__(self):\n",
    "        self.pipeline = None\n",
    "\n",
    "    def build_preprocessor(self, X):\n",
    "        num = X.select_dtypes(include=\"number\").columns.tolist()\n",
    "        cat = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "        num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "        cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "        return ColumnTransformer([(\"num\", num_pipe, num), (\"cat\", cat_pipe, cat)])\n",
    "\n",
    "    def build_pipeline(self, preprocessor):\n",
    "        self.pipeline = Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", RandomForestClassifier(**CONFIG[\"model_params\"]))\n",
    "        ])\n",
    "        return self.pipeline\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        return self.pipeline\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        preds = self.pipeline.predict(X_test)\n",
    "        print(classification_report(y_test, preds))\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\",\"Yes\"], yticklabels=[\"No\",\"Yes\"])\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(\"output/charts/confusion_matrix.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Feature Importance\n",
    "        model = self.pipeline.named_steps[\"model\"]\n",
    "        importances = model.feature_importances_\n",
    "        features = self.pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "        imp_df = pd.DataFrame({\"feature\": features, \"importance\": importances}).sort_values(by=\"importance\", ascending=False).head(15)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=\"importance\", y=\"feature\", data=imp_df)\n",
    "        plt.title(\"Top Features - Random Forest\")\n",
    "        plt.savefig(\"output/charts/feature_importance.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def predict_single(self, sample: dict):\n",
    "        df = pd.DataFrame([sample])\n",
    "        pred = self.pipeline.predict(df)[0]\n",
    "        proba = self.pipeline.predict_proba(df)[0][1]\n",
    "        # Chart: Prediction probability\n",
    "        plt.figure(figsize=(4,4))\n",
    "        sns.barplot(x=[\"Not Churn\",\"Churn\"], y=[1-proba, proba])\n",
    "        plt.title(\"Prediction Probability\")\n",
    "        plt.savefig(\"output/charts/single_prediction.png\")\n",
    "        plt.close()\n",
    "        return {\"prediction\": \"Churn\" if pred==1 else \"Not Churn\", \"probability\": proba}\n",
    "\n",
    "\n",
    "class TrainingPipeline:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_pipeline = DataPipeline(data_path)\n",
    "        self.model_pipeline = ModelPipeline()\n",
    "        self.pipeline = None\n",
    "\n",
    "    def run(self):\n",
    "        df = self.data_pipeline.load_data()\n",
    "        df = self.data_pipeline.handle_missing_values()\n",
    "        self.data_pipeline.detect_outliers(\"MonthlyCharges\")\n",
    "        df = self.data_pipeline.add_tenure_bins()\n",
    "        X_train, X_test, y_train, y_test = self.data_pipeline.split_data()\n",
    "\n",
    "        pre = self.model_pipeline.build_preprocessor(X_train)\n",
    "        self.model_pipeline.build_pipeline(pre)\n",
    "        self.model_pipeline.train(X_train, y_train)\n",
    "        self.model_pipeline.evaluate(X_test, y_test)\n",
    "\n",
    "        # Save model\n",
    "        joblib.dump(self.model_pipeline.pipeline, \"output/artifacts/churn_model.pkl\")\n",
    "        logging.info(\" Model saved to output/artifacts/churn_model.pkl\")\n",
    "\n",
    "        # Chart: Churn Rate in Test\n",
    "        churn_rate = y_test.mean()\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.barplot(x=[\"Not Churn\",\"Churn\"], y=[1-churn_rate, churn_rate])\n",
    "        plt.title(\"Churn Rate - Test Set\")\n",
    "        plt.savefig(\"output/charts/test_churn_rate.png\")\n",
    "        plt.close()\n",
    "\n",
    "        self.pipeline = self.model_pipeline.pipeline\n",
    "        return self.pipeline, (X_test, y_test)\n",
    "\n",
    "    def streaming_inference(self, samples):\n",
    "        results = []\n",
    "        for s in samples:\n",
    "            res = self.model_pipeline.predict_single(s)\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "        return results\n",
    "\n",
    "\n",
    "def validate_pipeline(pipeline, X_test, y_test):\n",
    "    assert hasattr(pipeline, \"predict\"), \"Pipeline missing predict()\"\n",
    "    assert len(X_test) == len(y_test), \"Mismatch in test shapes\"\n",
    "    preds = pipeline.predict(X_test)\n",
    "    assert set(np.unique(preds)).issubset({0,1}), \"Unexpected prediction values\"\n",
    "    logging.info(\" Pipeline validation passed.\")\n",
    "\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_file = ('Raw/MiniProjectS.csv')  # Changed from Processed/MiniProjectS_Model_Evaluation.csv\n",
    "    assert os.path.exists(data_file), \"CSV file not found!\"\n",
    "\n",
    "    trainer = TrainingPipeline(data_file)\n",
    "    pipeline, test_data = trainer.run()\n",
    "\n",
    "    # Streaming inference example\n",
    "    sample = {\n",
    "        \"gender\":\"Female\",\"SeniorCitizen\":0,\"Partner\":\"Yes\",\"Dependents\":\"No\",\"tenure\":5,\n",
    "        \"PhoneService\":\"Yes\",\"MultipleLines\":\"No\",\"InternetService\":\"Fiber optic\",\n",
    "        \"OnlineSecurity\":\"No\",\"OnlineBackup\":\"No\",\"DeviceProtection\":\"No\",\"TechSupport\":\"No\",\n",
    "        \"StreamingTV\":\"Yes\",\"StreamingMovies\":\"Yes\",\"Contract\":\"Month-to-month\",\n",
    "        \"PaperlessBilling\":\"Yes\",\"PaymentMethod\":\"Electronic check\",\n",
    "        \"MonthlyCharges\":70.0,\"TotalCharges\":300.0,\"TenureCategory\":\"New\"\n",
    "    }\n",
    "    trainer.streaming_inference([sample])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71697e30",
   "metadata": {},
   "source": [
    "# 4.2 Pipeline Implementation Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a36d3b",
   "metadata": {},
   "source": [
    "4.2.1 Code Structure Standards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd429ae",
   "metadata": {},
   "source": [
    "Object-Oriented Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "32eae53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class BasePipeline:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def log(self, message: str):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def handle_error(self, e: Exception):\n",
    "        self.logger.error(f\"Error in {self.__class__.__name__}: {str(e)}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb91ada",
   "metadata": {},
   "source": [
    "Strategy Pattern (Encoding, Scaling, Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "b98df486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "class EncodingStrategy:\n",
    "    def one_hot(self): return OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    def ordinal(self): return OrdinalEncoder()\n",
    "\n",
    "class ScalingStrategy:\n",
    "    def standard(self): return StandardScaler()\n",
    "    def minmax(self): return MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ba0de03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncodingStrategy().one_hot()\n",
    "scaler = ScalingStrategy().standard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f71a4",
   "metadata": {},
   "source": [
    "Load Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "29cb8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    CONFIG = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e876052",
   "metadata": {},
   "source": [
    " Error Handling + Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "167a3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst compactness  worst concavity  \\\n",
      "0                 0.07871  ...             0.6656           0.7119   \n",
      "1                 0.05667  ...             0.1866           0.2416   \n",
      "2                 0.05999  ...             0.4245           0.4504   \n",
      "3                 0.09744  ...             0.8663           0.6869   \n",
      "4                 0.05883  ...             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  target  \\\n",
      "0                0.2654          0.4601                  0.11890       0   \n",
      "1                0.1860          0.2750                  0.08902       0   \n",
      "2                0.2430          0.3613                  0.08758       0   \n",
      "3                0.2575          0.6638                  0.17300       0   \n",
      "4                0.1625          0.2364                  0.07678       0   \n",
      "\n",
      "   Churn_Prob     Retention_Segment  Retention_Priority  Risk_Segment  \n",
      "0    0.353677  Medium-Risk (Engage)              Medium           Low  \n",
      "1    0.248558     Low-Risk (Retain)                 Low           Low  \n",
      "2    0.415959  Medium-Risk (Engage)              Medium        Medium  \n",
      "3    0.159968     Low-Risk (Retain)                 Low           Low  \n",
      "4    0.550283  Medium-Risk (Engage)                High        Medium  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "(569, 35)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 35 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int64  \n",
      " 31  Churn_Prob               569 non-null    float64\n",
      " 32  Retention_Segment        569 non-null    object \n",
      " 33  Retention_Priority       569 non-null    object \n",
      " 34  Risk_Segment             569 non-null    object \n",
      "dtypes: float64(31), int64(1), object(3)\n",
      "memory usage: 155.7+ KB\n"
     ]
    }
   ],
   "source": [
    "class DataIngestion(BasePipeline):\n",
    "    def load_data(self, path: str):\n",
    "        try:\n",
    "            self.log(\"Loading dataset...\")\n",
    "            df = pd.read_csv(path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.handle_error(e)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2e445",
   "metadata": {},
   "source": [
    "Type Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "48420ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 35)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 35 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int64  \n",
      " 31  Churn_Prob               569 non-null    float64\n",
      " 32  Retention_Segment        569 non-null    object \n",
      " 33  Retention_Priority       569 non-null    object \n",
      " 34  Risk_Segment             569 non-null    object \n",
      "dtypes: float64(31), int64(1), object(3)\n",
      "memory usage: 155.7+ KB\n"
     ]
    }
   ],
   "source": [
    "def load_data(self, path: str) -> pd.DataFrame:\n",
    "    print(df.head())\n",
    "print(df.shape)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a851d",
   "metadata": {},
   "source": [
    "# 4.2.2 Data Pipeline Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e4b9b",
   "metadata": {},
   "source": [
    "Configurable Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "df18910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656af459",
   "metadata": {},
   "source": [
    " Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b70ae831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation(BasePipeline):\n",
    "    def validate(self, df: pd.DataFrame):\n",
    "        assert not df.empty, \"Dataset is empty!\"\n",
    "        assert \"customerID\" in df.columns, \"customerID missing!\"\n",
    "        self.log(\"Data validation passed.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caca5c0",
   "metadata": {},
   "source": [
    "Pipeline Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "47d6fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = Pipeline(steps=[\n",
    "    (\"encode\", EncodingStrategy().one_hot()),\n",
    "    (\"scale\", ScalingStrategy().standard())\n",
    "])\n",
    "\n",
    "joblib.dump(preprocessor, \"output/artifacts/preprocessor.pkl\")\n",
    "pre = joblib.load(\"output/artifacts/preprocessor.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c5b67",
   "metadata": {},
   "source": [
    "Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "e36ab845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290484f",
   "metadata": {},
   "source": [
    "# 4.2.3 Training Pipeline Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0725ea",
   "metadata": {},
   "source": [
    "Multiple Model Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "42cb48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "class ModelFactory:\n",
    "    def get_model(self, model_name: str, params: dict):\n",
    "        if model_name == \"random_forest\":\n",
    "            return RandomForestClassifier(**params)\n",
    "        elif model_name == \"gradient_boost\":\n",
    "            return GradientBoostingClassifier(**params)\n",
    "        elif model_name == \"adaboost\":\n",
    "            return AdaBoostClassifier(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not supported\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c904c7c",
   "metadata": {},
   "source": [
    " Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "effcdadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.85      1033\n",
      "           1       0.00      0.00      0.00       374\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.37      0.50      0.42      1407\n",
      "weighted avg       0.54      0.73      0.62      1407\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Model saved to output/artifacts/churn_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__max_depth': 10, 'classifier__n_estimators': 200}\n",
      "Best score: 0.2062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Option 1: Use the existing pipeline\n",
    "trainer = TrainingPipeline('Raw/MiniProjectS.csv')\n",
    "pipeline, test_data = trainer.run()\n",
    "X_test, y_test = test_data\n",
    "\n",
    "# Get the training data from the data pipeline\n",
    "df = trainer.data_pipeline.df\n",
    "X = df.drop(columns=[CONFIG[\"target\"], \"customerID\"])\n",
    "y = df[CONFIG[\"target\"]].apply(lambda x: 1 if str(x).lower()==\"yes\" else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=CONFIG[\"test_size\"], \n",
    "    random_state=CONFIG[\"random_state\"], stratify=y\n",
    ")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "num_cols = X_train.select_dtypes(include=\"number\").columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols), \n",
    "    (\"cat\", cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "# Create full pipeline with preprocessing\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# use GridSearchCV with the full pipeline\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200], \n",
    "    \"classifier__max_depth\": [5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(full_pipeline, param_grid, cv=3, scoring=\"f1\")\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d810c",
   "metadata": {},
   "source": [
    "Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "e82f6563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest: F1 Score = 0.5416\n",
      "gradient_boost: F1 Score = 0.5667\n",
      "adaboost: F1 Score = 0.5602\n",
      "\n",
      "Best model: gradient_boost with F1 Score: 0.5667\n"
     ]
    }
   ],
   "source": [
    "# Multiple Model Support with Proper Preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class ModelFactory:\n",
    "    def get_model(self, model_name: str, params: dict):\n",
    "        if model_name == \"random_forest\":\n",
    "            return RandomForestClassifier(**params)\n",
    "        elif model_name == \"gradient_boost\":\n",
    "            return GradientBoostingClassifier(**params)\n",
    "        elif model_name == \"adaboost\":\n",
    "            return AdaBoostClassifier(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not supported\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "num_cols = X_train.select_dtypes(include=\"number\").columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols), \n",
    "    (\"cat\", cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "# Test multiple models with preprocessing\n",
    "results = {}\n",
    "for model_name in [\"random_forest\", \"gradient_boost\", \"adaboost\"]:\n",
    "    # Create full pipeline with preprocessing and model\n",
    "    full_pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", ModelFactory().get_model(model_name, {}))\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    full_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = full_pipeline.predict(X_test)\n",
    "    results[model_name] = f1_score(y_test, preds)\n",
    "\n",
    "# Display results\n",
    "for model_name, score in results.items():\n",
    "    print(f\"{model_name}: F1 Score = {score:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=results.get)\n",
    "print(f\"\\nBest model: {best_model_name} with F1 Score: {results[best_model_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca10808",
   "metadata": {},
   "source": [
    "Model Persistence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1076b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(best_model, \"output/artifacts/best_model.pkl\")\n",
    "\n",
    "with open(\"output/artifacts/evaluation_report.txt\", \"w\") as f:\n",
    "    f.write(str(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495652b2",
   "metadata": {},
   "source": [
    "# 4.2.4 Inference Pipeline Features\n",
    "\n",
    "\n",
    "Batch Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b6fbf",
   "metadata": {},
   "source": [
    "Single Sample Prediction\n",
    "\n",
    "Input Validation\n",
    "\n",
    "Probability Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5eb6a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Predictions: [0 1 0 0 0]\n",
      "Single Prediction: [1]\n",
      "Input Validation Error: Expected 2D array, got 1D array instead:\n",
      "array=[0.4303059  0.20052473 0.49159455 0.06420894 0.5819714 ].\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
      "Prediction Probabilities:\n",
      " [[0.63 0.37]\n",
      " [0.32 0.68]\n",
      " [0.63 0.37]\n",
      " [0.88 0.12]\n",
      " [0.69 0.31]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "model = joblib.load(\"best_model.pkl\")\n",
    "\n",
    "\n",
    "X_batch = np.random.rand(5, 5)  \n",
    "batch_preds = model.predict(X_batch)\n",
    "print(\"Batch Predictions:\", batch_preds)\n",
    "\n",
    "\n",
    "X_single = np.random.rand(1, 5) \n",
    "single_pred = model.predict(X_single)\n",
    "print(\"Single Prediction:\", single_pred)\n",
    "\n",
    "try:\n",
    "    X_wrong = np.random.rand(5) \n",
    "    model.predict(X_wrong)\n",
    "except Exception as e:\n",
    "    print(\"Input Validation Error:\", str(e))\n",
    "\n",
    "\n",
    "probs = model.predict_proba(X_batch)\n",
    "print(\"Prediction Probabilities:\\n\", probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d672a",
   "metadata": {},
   "source": [
    "# 4.3 Pipeline Testing and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273008f3",
   "metadata": {},
   "source": [
    "\n",
    "Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649eb42",
   "metadata": {},
   "source": [
    "\n",
    "Integration Tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30c8b7",
   "metadata": {},
   "source": [
    "Data Validation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a693806",
   "metadata": {},
   "source": [
    "Model Performance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "da9fcffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Test: PASSED\n",
      "Integration Test Accuracy: 0.65\n",
      "Data Validation Test: PASSED\n",
      "Model Performance Test: PASSED (Accuracy >= 0.6)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dummy dataset\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Unit test: check model is trained\n",
    "def test_model_training():\n",
    "    assert model is not None, \"Model not initialized\"\n",
    "    assert hasattr(model, \"predict\"), \"Model has no predict method\"\n",
    "    print(\"Model Training Test: PASSED\")\n",
    "\n",
    "test_model_training()\n",
    "\n",
    "\n",
    "\n",
    "# Integration test: end-to-end pipeline\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Integration Test Accuracy:\", acc)\n",
    "\n",
    "\n",
    "\n",
    "# Data validation: check for NaN values\n",
    "assert not np.isnan(X_train).any(), \"NaN values in training data\"\n",
    "assert not np.isnan(X_test).any(), \"NaN values in testing data\"\n",
    "print(\"Data Validation Test: PASSED\")\n",
    "\n",
    "\n",
    "# Model performance threshold\n",
    "threshold = 0.6\n",
    "if acc >= threshold:\n",
    "    print(\"Model Performance Test: PASSED (Accuracy >= 0.6)\")\n",
    "else:\n",
    "    print(\"Model Performance Test: FAILED (Accuracy < 0.6)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89035717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully! Shape: (569, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from your existing processed file\n",
    "df = pd.read_csv('Processed/MiniProjectS_Model_Evaluation.csv')\n",
    "\n",
    "\n",
    "df.to_csv('Processed/MiniProjectS_Production_Ready.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3ce556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in the dataset:\n",
      "['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', 'target', 'Churn_Prob', 'Retention_Segment', 'Retention_Priority', 'Risk_Segment']\n",
      "\n",
      "First few rows:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst compactness  worst concavity  \\\n",
      "0                 0.07871  ...             0.6656           0.7119   \n",
      "1                 0.05667  ...             0.1866           0.2416   \n",
      "2                 0.05999  ...             0.4245           0.4504   \n",
      "3                 0.09744  ...             0.8663           0.6869   \n",
      "4                 0.05883  ...             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  target  \\\n",
      "0                0.2654          0.4601                  0.11890       0   \n",
      "1                0.1860          0.2750                  0.08902       0   \n",
      "2                0.2430          0.3613                  0.08758       0   \n",
      "3                0.2575          0.6638                  0.17300       0   \n",
      "4                0.1625          0.2364                  0.07678       0   \n",
      "\n",
      "   Churn_Prob     Retention_Segment  Retention_Priority  Risk_Segment  \n",
      "0    0.353677  Medium-Risk (Engage)              Medium           Low  \n",
      "1    0.248558     Low-Risk (Retain)                 Low           Low  \n",
      "2    0.415959  Medium-Risk (Engage)              Medium        Medium  \n",
      "3    0.159968     Low-Risk (Retain)                 Low           Low  \n",
      "4    0.550283  Medium-Risk (Engage)                High        Medium  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "Data shape: (569, 35)\n"
     ]
    }
   ],
   "source": [
    "# First, let's check what columns are actually available in your data\n",
    "print(\"Available columns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData shape:\", df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
